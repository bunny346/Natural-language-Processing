{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOipRNMOM8xDEFoU0fb3awT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bunny346/Natural-language-Processing/blob/main/NLP_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8LxNDeuFEnW",
        "outputId": "b99ece7d-1679-4c20-fe2f-8d7f626836be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ANN with TF-IDF ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram -> Train: 0.9966, Test: 0.8857\n",
            "Unigram+Bigram -> Train: 0.9958, Test: 0.8813\n",
            "Unigram+Bigram+Trigram -> Train: 0.9959, Test: 0.8813\n",
            "\n",
            "=== LSTM with Embeddings ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM -> Train: 0.9953, Test: 0.9006\n",
            "\n",
            "=== Analysis ===\n",
            "Bigrams usually outperform unigrams because phrases like 'fire alarm' or 'flood warning' carry more meaning than single words.\n",
            "Trigrams often add sparsity and risk of overfitting.\n",
            "LSTM already learns sequential dependencies, so explicit bigrams/trigrams help less.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "df = pd.read_csv(\"/content/archive (5).zip\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = [w for w in text.split() if w not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['clean'] = df['text'].astype(str).apply(clean_text)\n",
        "\n",
        "X = df['clean']\n",
        "y = df['target'] # Corrected column name from 'label' to 'target'\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def run_ann(ngram_range):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=10000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train_tfidf.toarray(), y_train, epochs=5, batch_size=64, verbose=0,\n",
        "              validation_data=(X_test_tfidf.toarray(), y_test))\n",
        "    train_acc = model.evaluate(X_train_tfidf.toarray(), y_train, verbose=0)[1]\n",
        "    test_acc = model.evaluate(X_test_tfidf.toarray(), y_test, verbose=0)[1]\n",
        "    return train_acc, test_acc\n",
        "\n",
        "print(\"\\n=== ANN with TF-IDF ===\")\n",
        "uni_acc = run_ann((1,1))\n",
        "print(\"Unigram -> Train: %.4f, Test: %.4f\" % uni_acc)\n",
        "bi_acc  = run_ann((1,2))\n",
        "print(\"Unigram+Bigram -> Train: %.4f, Test: %.4f\" % bi_acc)\n",
        "tri_acc = run_ann((1,3))\n",
        "print(\"Unigram+Bigram+Trigram -> Train: %.4f, Test: %.4f\" % tri_acc)\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQ_LEN = 100\n",
        "EMB_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_SEQ_LEN)\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAX_SEQ_LEN)\n",
        "\n",
        "def run_lstm():\n",
        "    model = Sequential([\n",
        "        Embedding(MAX_NB_WORDS, EMB_DIM, input_length=MAX_SEQ_LEN),\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X_train_seq, y_train, epochs=5, batch_size=64,\n",
        "              validation_data=(X_test_seq, y_test), verbose=0)\n",
        "    train_acc = model.evaluate(X_train_seq, y_train, verbose=0)[1]\n",
        "    test_acc = model.evaluate(X_test_seq, y_test, verbose=0)[1]\n",
        "    return train_acc, test_acc\n",
        "\n",
        "print(\"\\n=== LSTM with Embeddings ===\")\n",
        "lstm_acc = run_lstm()\n",
        "print(\"LSTM -> Train: %.4f, Test: %.4f\" % lstm_acc)\n",
        "\n",
        "print(\"\\n=== Analysis ===\")\n",
        "print(\"Bigrams usually outperform unigrams because phrases like 'fire alarm' or 'flood warning' carry more meaning than single words.\")\n",
        "print(\"Trigrams often add sparsity and risk of overfitting.\")\n",
        "print(\"LSTM already learns sequential dependencies, so explicit bigrams/trigrams help less.\")"
      ]
    }
  ]
}