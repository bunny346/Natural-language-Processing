{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzi0g3NlPb8+QjO1BByD1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bunny346/Natural-learning-processing/blob/main/NLP_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdHx3V6kGGzG",
        "outputId": "bd0bd203-814a-40df-f675-24617f9d22bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "100%|██████████| 11370/11370 [00:00<00:00, 54160.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression-TFIDF -> Acc: 0.8544, Precision: 0.5849, Recall: 0.7494, F1: 0.6570\n",
            "LinearSVC-TFIDF -> Acc: 0.8641, Precision: 0.6228, Recall: 0.6832, F1: 0.6516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 24ms/step - accuracy: 0.4782 - loss: 0.6947 - val_accuracy: 0.8615 - val_loss: 0.6835\n",
            "Epoch 2/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.6354 - loss: 0.6677 - val_accuracy: 0.7286 - val_loss: 0.5334\n",
            "Epoch 3/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.8510 - loss: 0.3750 - val_accuracy: 0.8879 - val_loss: 0.2966\n",
            "Epoch 4/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.9171 - loss: 0.2344 - val_accuracy: 0.8769 - val_loss: 0.3271\n",
            "Epoch 5/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9382 - loss: 0.1917 - val_accuracy: 0.8846 - val_loss: 0.3085\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
            "MLP-AverageEmb -> Acc: 0.8808, Precision: 0.7088, Recall: 0.6099, F1: 0.6557\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.5639 - loss: 0.6659 - val_accuracy: 0.8703 - val_loss: 0.3475\n",
            "Epoch 2/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 69ms/step - accuracy: 0.8760 - loss: 0.3323 - val_accuracy: 0.8363 - val_loss: 0.4226\n",
            "Epoch 3/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 65ms/step - accuracy: 0.9652 - loss: 0.1018 - val_accuracy: 0.8462 - val_loss: 0.4839\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
            "CNN1D -> Acc: 0.8571, Precision: 0.6021, Recall: 0.6832, F1: 0.6401\n",
            "Epoch 1/8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 212ms/step - accuracy: 0.6194 - loss: 0.6517 - val_accuracy: 0.8165 - val_loss: 0.4524\n",
            "Epoch 2/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 199ms/step - accuracy: 0.8913 - loss: 0.2994 - val_accuracy: 0.8407 - val_loss: 0.4083\n",
            "Epoch 3/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 217ms/step - accuracy: 0.9607 - loss: 0.1270 - val_accuracy: 0.8473 - val_loss: 0.4766\n",
            "Epoch 4/8\n",
            "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 208ms/step - accuracy: 0.9856 - loss: 0.0572 - val_accuracy: 0.8505 - val_loss: 0.5333\n",
            "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 164ms/step\n",
            "BiLSTM -> Acc: 0.8276, Precision: 0.5264, Recall: 0.7305, F1: 0.6119\n",
            "\n",
            "=== RESULTS SUMMARY ===\n",
            "                   model  accuracy  precision   recall       f1\n",
            "LogisticRegression-TFIDF  0.854442   0.584871 0.749409 0.656995\n",
            "          MLP-AverageEmb  0.880827   0.708791 0.609929 0.655654\n",
            "         LinearSVC-TFIDF  0.864116   0.622845 0.683215 0.651635\n",
            "                   CNN1D  0.857080   0.602083 0.683215 0.640089\n",
            "                  BiLSTM  0.827617   0.526405 0.730496 0.611881\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "MAX_NUM_WORDS = 30000\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 8\n",
        "VALIDATION_SPLIT = 0.1\n",
        "\n",
        "df = pd.read_csv('/content/archive (1).zip')\n",
        "df = df[['text','target']].dropna().reset_index(drop=True)\n",
        "\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def clean_tweet(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+', ' ', text)\n",
        "    text = re.sub(r'http\\S+|www.\\S+', ' ', text)\n",
        "    text = re.sub(r'#\\w+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS and len(t)>1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "tqdm.pandas()\n",
        "df['clean_text'] = df['text'].progress_apply(clean_tweet)\n",
        "\n",
        "X = df['clean_text'].values\n",
        "y = df['target'].values\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
        "\n",
        "cv = CountVectorizer(max_features=20000, ngram_range=(1,2))\n",
        "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
        "X_train_cv = cv.fit_transform(X_train_raw)\n",
        "X_test_cv = cv.transform(X_test_raw)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_raw)\n",
        "X_test_tfidf = tfidf.transform(X_test_raw)\n",
        "\n",
        "def eval_and_print(model, X_test, y_test, name):\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
        "    print(f\"{name} -> Acc: {acc:.4f}, Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
        "    return {'model': name, 'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1}\n",
        "\n",
        "results = []\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_SEED)\n",
        "lr.fit(X_train_tfidf, y_train)\n",
        "results.append(eval_and_print(lr, X_test_tfidf, y_test, \"LogisticRegression-TFIDF\"))\n",
        "\n",
        "svm = LinearSVC(max_iter=2000, class_weight='balanced', random_state=RANDOM_SEED)\n",
        "svm.fit(X_train_tfidf, y_train)\n",
        "results.append(eval_and_print(svm, X_test_tfidf, y_test, \"LinearSVC-TFIDF\"))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train_raw)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_raw)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_raw)\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "def compute_metrics_from_probs(probs, y_true, threshold=0.5):\n",
        "    y_pred = (probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
        "    return acc, p, r, f1\n",
        "\n",
        "def train_and_evaluate_keras(model, X_tr, y_tr, X_te, y_te, name, epochs=EPOCHS, batch_size=BATCH_SIZE):\n",
        "    es = callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    history = model.fit(X_tr, y_tr, validation_split=VALIDATION_SPLIT, epochs=epochs, batch_size=batch_size, class_weight=class_weight_dict, callbacks=[es], verbose=1)\n",
        "    probs = model.predict(X_te, batch_size=128).ravel()\n",
        "    acc, p, r, f1 = compute_metrics_from_probs(probs, y_te)\n",
        "    print(f\"{name} -> Acc: {acc:.4f}, Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\")\n",
        "    return {'model': name, 'accuracy': acc, 'precision': p, 'recall': r, 'f1': f1, 'history': history}\n",
        "\n",
        "def build_mlp_avg(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "mlp_model = build_mlp_avg(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(mlp_model, X_train_pad, y_train, X_test_pad, y_test, \"MLP-AverageEmb\"))\n",
        "\n",
        "def build_cnn1d(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    convs = []\n",
        "    for fsz in [2,3,4]:\n",
        "        c = layers.Conv1D(filters=128, kernel_size=fsz, activation='relu')(x)\n",
        "        c = layers.GlobalMaxPooling1D()(c)\n",
        "        convs.append(c)\n",
        "    x = layers.concatenate(convs)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "cnn_model = build_cnn1d(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(cnn_model, X_train_pad, y_train, X_test_pad, y_test, \"CNN1D\"))\n",
        "\n",
        "def build_lstm(vocab_size, embedding_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH):\n",
        "    inp = layers.Input(shape=(input_length,))\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length)(inp)\n",
        "    x = layers.SpatialDropout1D(0.2)(x)\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    out = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "lstm_model = build_lstm(vocab_size=vocab_size)\n",
        "results.append(train_and_evaluate_keras(lstm_model, X_train_pad, y_train, X_test_pad, y_test, \"BiLSTM\"))\n",
        "\n",
        "print(\"\\n=== RESULTS SUMMARY ===\")\n",
        "res_df = pd.DataFrame(results)\n",
        "print(res_df[['model','accuracy','precision','recall','f1']].sort_values(by='f1', ascending=False).to_string(index=False))\n",
        "res_df.to_csv('model_comparison_results.csv', index=False)\n"
      ]
    }
  ]
}